{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a911a8-99cc-479a-b220-87e50b6dcb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:15<00:00, 64.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset selesai di-generate! Terdapat 1000 gambar dan anotasi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm  # Untuk progress bar\n",
    "\n",
    "# Folder dataset ikon dan folder untuk background\n",
    "dataset_dir = 'dataset'\n",
    "background_dir = 'backgrounds'\n",
    "output_dir = 'generated_dataset'\n",
    "annotations_dir = os.path.join(output_dir, 'annotations')\n",
    "images_dir = os.path.join(output_dir, 'images')\n",
    "\n",
    "# Buat folder output jika belum ada\n",
    "os.makedirs(annotations_dir, exist_ok=True)\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "# Jumlah minimal kelas dalam satu gambar\n",
    "num_classes_per_image = 5\n",
    "num_images_to_generate = 1000\n",
    "\n",
    "# Fungsi untuk membuat background dinamis (acak atau polos)\n",
    "def generate_random_background(width, height):\n",
    "    if random.random() > 0.5:\n",
    "        # Buat background warna acak\n",
    "        background = np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)\n",
    "    else:\n",
    "        # Buat background polos dengan warna tertentu\n",
    "        solid_colors = [\n",
    "            (0, 0, 0),        # Hitam\n",
    "            (255, 255, 255),  # Putih\n",
    "            (128, 128, 128),  # Abu-abu\n",
    "            (255, 0, 0),      # Merah\n",
    "            (0, 255, 0),      # Hijau\n",
    "            (0, 0, 255)       # Biru\n",
    "        ]\n",
    "        chosen_color = random.choice(solid_colors)\n",
    "        background = np.full((height, width, 3), chosen_color, dtype=np.uint8)\n",
    "\n",
    "    return background\n",
    "\n",
    "# Fungsi untuk mengecek apakah ada tumpang tindih antara bounding box\n",
    "def is_overlapping(new_bbox, existing_bboxes):\n",
    "    for bbox in existing_bboxes:\n",
    "        if not (new_bbox['xmax'] <= bbox['xmin'] or\n",
    "                new_bbox['xmin'] >= bbox['xmax'] or\n",
    "                new_bbox['ymax'] <= bbox['ymin'] or\n",
    "                new_bbox['ymin'] >= bbox['ymax']):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Fungsi untuk menggabungkan ikon dengan background\n",
    "def add_icons_to_background(background, icons):\n",
    "    bg_h, bg_w, _ = background.shape\n",
    "    bounding_boxes = []\n",
    "\n",
    "    for icon in icons:\n",
    "        icon_h, icon_w, _ = icon.shape\n",
    "\n",
    "        # Resize ikon ke ukuran acak\n",
    "        scale_factor = random.uniform(0.5, 1.5)  # Ukuran antara 50% hingga 150% dari ukuran aslinya\n",
    "        new_w = int(icon_w * scale_factor)\n",
    "        new_h = int(icon_h * scale_factor)\n",
    "        icon = cv2.resize(icon, (new_w, new_h))\n",
    "\n",
    "        # Tentukan posisi acak di background dengan pengecekan tumpang tindih\n",
    "        attempts = 0\n",
    "        max_attempts = 50\n",
    "\n",
    "        # Pastikan ikon tidak lebih besar dari background\n",
    "        if new_w > bg_w or new_h > bg_h:\n",
    "            new_w = min(new_w, bg_w)\n",
    "            new_h = min(new_h, bg_h)\n",
    "            icon = cv2.resize(icon, (new_w, new_h))\n",
    "\n",
    "        while attempts < max_attempts:\n",
    "            x_offset = random.randint(0, bg_w - new_w)\n",
    "            y_offset = random.randint(0, bg_h - new_h)\n",
    "\n",
    "            new_bbox = {\n",
    "                'xmin': x_offset,\n",
    "                'ymin': y_offset,\n",
    "                'xmax': x_offset + new_w,\n",
    "                'ymax': y_offset + new_h\n",
    "            }\n",
    "\n",
    "            # Jika tidak ada tumpang tindih, tambahkan ikon\n",
    "            if not is_overlapping(new_bbox, bounding_boxes):\n",
    "                background[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = icon\n",
    "                bounding_boxes.append(new_bbox)\n",
    "                break\n",
    "            attempts += 1\n",
    "\n",
    "    return background, bounding_boxes\n",
    "\n",
    "# Fungsi untuk menyimpan anotasi bounding box dalam format JSON\n",
    "def save_annotation(annotation_data, annotation_path):\n",
    "    with open(annotation_path, 'w') as f:\n",
    "        json.dump(annotation_data, f, indent=4)\n",
    "\n",
    "# Loop untuk membuat 1000 dataset\n",
    "for i in tqdm(range(num_images_to_generate)):\n",
    "    # Pilih background\n",
    "    if os.path.exists(background_dir) and len(os.listdir(background_dir)) > 0:\n",
    "        background_name = random.choice(os.listdir(background_dir))\n",
    "        background_path = os.path.join(background_dir, background_name)\n",
    "        background = cv2.imread(background_path)\n",
    "    else:\n",
    "        # Buat background acak jika tidak ada background di folder\n",
    "        background = generate_random_background(800, 600)\n",
    "\n",
    "    # Pilih ikon dari minimal 5 kelas berbeda\n",
    "    class_names = random.sample(os.listdir(dataset_dir), num_classes_per_image)\n",
    "    icons = []\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        image_name = random.choice(os.listdir(class_dir))\n",
    "        image_path = os.path.join(class_dir, image_name)\n",
    "        icon = cv2.imread(image_path)\n",
    "        icons.append(icon)\n",
    "\n",
    "    # Gabungkan ikon-ikon dengan background\n",
    "    combined_image, bboxes = add_icons_to_background(background, icons)\n",
    "\n",
    "    # Simpan gambar hasil\n",
    "    image_filename = f'image_{i:04d}.jpg'\n",
    "    image_path = os.path.join(images_dir, image_filename)\n",
    "    cv2.imwrite(image_path, combined_image)\n",
    "\n",
    "    # Buat anotasi untuk gambar ini\n",
    "    annotation_data = {\n",
    "        'image': image_filename,\n",
    "        'bboxes': [],\n",
    "        'classes': []\n",
    "    }\n",
    "    \n",
    "    for bbox, class_name in zip(bboxes, class_names):\n",
    "        annotation_data['bboxes'].append(bbox)\n",
    "        annotation_data['classes'].append(class_name)\n",
    "\n",
    "    # Simpan anotasi\n",
    "    annotation_filename = f'annotation_{i:04d}.json'\n",
    "    annotation_path = os.path.join(annotations_dir, annotation_filename)\n",
    "    save_annotation(annotation_data, annotation_path)\n",
    "\n",
    "print(f\"Dataset selesai di-generate! Terdapat {num_images_to_generate} gambar dan anotasi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "834ad058-43e0-41e1-8069-956e9d829020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /home/user/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:00<00:00, 216MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0], Loss: 5.723782539367676\n",
      "Epoch [1/10], Step [10], Loss: 0.27422013878822327\n",
      "Epoch [1/10], Step [20], Loss: 0.11937808990478516\n",
      "Epoch [1/10], Step [30], Loss: 0.12543217837810516\n",
      "Epoch [1/10], Step [40], Loss: 0.06396651268005371\n",
      "Epoch [1/10], Step [50], Loss: 0.06770409643650055\n",
      "Epoch [1/10], Step [60], Loss: 0.04129418358206749\n",
      "Epoch [1/10], Step [70], Loss: 0.04104039445519447\n",
      "Epoch [1/10], Step [80], Loss: 0.03679511696100235\n",
      "Epoch [1/10], Step [90], Loss: 0.02974880486726761\n",
      "Epoch [1/10], Step [100], Loss: 0.027816612273454666\n",
      "Epoch [1/10], Step [110], Loss: 0.0233454667031765\n",
      "Epoch [1/10], Step [120], Loss: 0.01906193420290947\n",
      "Epoch [1/10], Step [130], Loss: 0.018896130844950676\n",
      "Epoch [1/10], Step [140], Loss: 0.017471032217144966\n",
      "Epoch [1/10], Step [150], Loss: 0.011233817785978317\n",
      "Epoch [1/10], Step [160], Loss: 0.013945864513516426\n",
      "Epoch [1/10], Step [170], Loss: 0.013210044242441654\n",
      "Epoch [1/10], Step [180], Loss: 0.019261514768004417\n",
      "Epoch [1/10], Step [190], Loss: 0.01397729478776455\n",
      "Epoch [1/10], Step [200], Loss: 0.009309832006692886\n",
      "Epoch [1/10], Step [210], Loss: 0.014101664535701275\n",
      "Epoch [1/10], Step [220], Loss: 0.008308219723403454\n",
      "Epoch [1/10], Step [230], Loss: 0.005973050836473703\n",
      "Epoch [1/10], Step [240], Loss: 0.004562832415103912\n",
      "Epoch [1/10], Step [250], Loss: 0.006046115420758724\n",
      "Epoch [1/10], Step [260], Loss: 0.004744691774249077\n",
      "Epoch [1/10], Step [270], Loss: 0.015511924400925636\n",
      "Epoch [1/10], Step [280], Loss: 0.020654097199440002\n",
      "Epoch [1/10], Step [290], Loss: 0.006177844479680061\n",
      "Epoch [1/10], Step [300], Loss: 0.003809048095718026\n",
      "Epoch [1/10], Step [310], Loss: 0.0045334999449551105\n",
      "Epoch [1/10], Step [320], Loss: 0.00513219740241766\n",
      "Epoch [1/10], Step [330], Loss: 0.0069507029838860035\n",
      "Epoch [1/10], Step [340], Loss: 0.004947734996676445\n",
      "Epoch [1/10], Step [350], Loss: 0.006469466723501682\n",
      "Epoch [1/10], Step [360], Loss: 0.004699171986430883\n",
      "Epoch [1/10], Step [370], Loss: 0.004798684269189835\n",
      "Epoch [1/10], Step [380], Loss: 0.0037556691095232964\n",
      "Epoch [1/10], Step [390], Loss: 0.0020273039117455482\n",
      "Epoch [1/10], Step [400], Loss: 0.0027222696226090193\n",
      "Epoch [1/10], Step [410], Loss: 0.0032501709647476673\n",
      "Epoch [1/10], Step [420], Loss: 0.004072632174938917\n",
      "Epoch [1/10], Step [430], Loss: 0.0035271889064460993\n",
      "Epoch [1/10], Step [440], Loss: 0.0026948812883347273\n",
      "Epoch [1/10], Step [450], Loss: 0.0036147560458630323\n",
      "Epoch [1/10], Step [460], Loss: 0.00607835128903389\n",
      "Epoch [1/10], Step [470], Loss: 0.005267328582704067\n",
      "Epoch [1/10], Step [480], Loss: 0.00358413509093225\n",
      "Epoch [1/10], Step [490], Loss: 0.00306001678109169\n",
      "Epoch [2/10], Step [0], Loss: 0.0033470450434833765\n",
      "Epoch [2/10], Step [10], Loss: 0.001591642270796001\n",
      "Epoch [2/10], Step [20], Loss: 0.00332453940063715\n",
      "Epoch [2/10], Step [30], Loss: 0.008781708776950836\n",
      "Epoch [2/10], Step [40], Loss: 0.003290667198598385\n",
      "Epoch [2/10], Step [50], Loss: 0.00473176222294569\n",
      "Epoch [2/10], Step [60], Loss: 0.00446169450879097\n",
      "Epoch [2/10], Step [70], Loss: 0.0032498815562576056\n",
      "Epoch [2/10], Step [80], Loss: 0.0016878889873623848\n",
      "Epoch [2/10], Step [90], Loss: 0.0011432453757151961\n",
      "Epoch [2/10], Step [100], Loss: 0.0034358156844973564\n",
      "Epoch [2/10], Step [110], Loss: 0.003518155077472329\n",
      "Epoch [2/10], Step [120], Loss: 0.0019814418628811836\n",
      "Epoch [2/10], Step [130], Loss: 0.003299173666164279\n",
      "Epoch [2/10], Step [140], Loss: 0.0018608559621497989\n",
      "Epoch [2/10], Step [150], Loss: 0.000987377017736435\n",
      "Epoch [2/10], Step [160], Loss: 0.0010796783026307821\n",
      "Epoch [2/10], Step [170], Loss: 0.0009782238630577922\n",
      "Epoch [2/10], Step [180], Loss: 0.0024266152177006006\n",
      "Epoch [2/10], Step [190], Loss: 0.002176601905375719\n",
      "Epoch [2/10], Step [200], Loss: 0.006867900490760803\n",
      "Epoch [2/10], Step [210], Loss: 0.0011311215348541737\n",
      "Epoch [2/10], Step [220], Loss: 0.002030404517427087\n",
      "Epoch [2/10], Step [230], Loss: 0.0010275584645569324\n",
      "Epoch [2/10], Step [240], Loss: 0.0012158185709267855\n",
      "Epoch [2/10], Step [250], Loss: 0.0119021600112319\n",
      "Epoch [2/10], Step [260], Loss: 0.004237966611981392\n",
      "Epoch [2/10], Step [270], Loss: 0.002338952384889126\n",
      "Epoch [2/10], Step [280], Loss: 0.0023998732212930918\n",
      "Epoch [2/10], Step [290], Loss: 0.004564685747027397\n",
      "Epoch [2/10], Step [300], Loss: 0.0015591408591717482\n",
      "Epoch [2/10], Step [310], Loss: 0.004253104329109192\n",
      "Epoch [2/10], Step [320], Loss: 0.0022253566421568394\n",
      "Epoch [2/10], Step [330], Loss: 0.0013286793837323785\n",
      "Epoch [2/10], Step [340], Loss: 0.0010216375812888145\n",
      "Epoch [2/10], Step [350], Loss: 0.0023737726733088493\n",
      "Epoch [2/10], Step [360], Loss: 0.0019720732234418392\n",
      "Epoch [2/10], Step [370], Loss: 0.004108314402401447\n",
      "Epoch [2/10], Step [380], Loss: 0.001528749824501574\n",
      "Epoch [2/10], Step [390], Loss: 0.001179367071017623\n",
      "Epoch [2/10], Step [400], Loss: 0.0023748367093503475\n",
      "Epoch [2/10], Step [410], Loss: 0.009403131902217865\n",
      "Epoch [2/10], Step [420], Loss: 0.0015314359916374087\n",
      "Epoch [2/10], Step [430], Loss: 0.0005549010820686817\n",
      "Epoch [2/10], Step [440], Loss: 0.00099803670309484\n",
      "Epoch [2/10], Step [450], Loss: 0.0014041701797395945\n",
      "Epoch [2/10], Step [460], Loss: 0.0008523543365299702\n",
      "Epoch [2/10], Step [470], Loss: 0.0009318166994489729\n",
      "Epoch [2/10], Step [480], Loss: 0.0014361814828589559\n",
      "Epoch [2/10], Step [490], Loss: 0.002222210168838501\n",
      "Epoch [3/10], Step [0], Loss: 0.01170047651976347\n",
      "Epoch [3/10], Step [10], Loss: 0.0006246923003345728\n",
      "Epoch [3/10], Step [20], Loss: 0.0008844828116707504\n",
      "Epoch [3/10], Step [30], Loss: 0.0009047635830938816\n",
      "Epoch [3/10], Step [40], Loss: 0.00041739700827747583\n",
      "Epoch [3/10], Step [50], Loss: 0.0005900597316212952\n",
      "Epoch [3/10], Step [60], Loss: 0.000621673883870244\n",
      "Epoch [3/10], Step [70], Loss: 0.00088473194045946\n",
      "Epoch [3/10], Step [80], Loss: 0.0005433687474578619\n",
      "Epoch [3/10], Step [90], Loss: 0.006610647775232792\n",
      "Epoch [3/10], Step [100], Loss: 0.004543889779597521\n",
      "Epoch [3/10], Step [110], Loss: 0.00891586858779192\n",
      "Epoch [3/10], Step [120], Loss: 0.0009449593489989638\n",
      "Epoch [3/10], Step [130], Loss: 0.0010938092600554228\n",
      "Epoch [3/10], Step [140], Loss: 0.0008297737222164869\n",
      "Epoch [3/10], Step [150], Loss: 0.0005656027933582664\n",
      "Epoch [3/10], Step [160], Loss: 0.0005721210036426783\n",
      "Epoch [3/10], Step [170], Loss: 0.005557942669838667\n",
      "Epoch [3/10], Step [180], Loss: 0.0029030470177531242\n",
      "Epoch [3/10], Step [190], Loss: 0.0016540844226256013\n",
      "Epoch [3/10], Step [200], Loss: 0.0008445371058769524\n",
      "Epoch [3/10], Step [210], Loss: 0.0005283536738716066\n",
      "Epoch [3/10], Step [220], Loss: 0.0005493719363585114\n",
      "Epoch [3/10], Step [230], Loss: 0.0007528893183916807\n",
      "Epoch [3/10], Step [240], Loss: 0.0006150064873509109\n",
      "Epoch [3/10], Step [250], Loss: 0.008938988670706749\n",
      "Epoch [3/10], Step [260], Loss: 0.001665729796513915\n",
      "Epoch [3/10], Step [270], Loss: 0.000602997955866158\n",
      "Epoch [3/10], Step [280], Loss: 0.0008028780575841665\n",
      "Epoch [3/10], Step [290], Loss: 0.010261096060276031\n",
      "Epoch [3/10], Step [300], Loss: 0.0024723645765334368\n",
      "Epoch [3/10], Step [310], Loss: 0.0010508415289223194\n",
      "Epoch [3/10], Step [320], Loss: 0.0012594768777489662\n",
      "Epoch [3/10], Step [330], Loss: 0.00216478924266994\n",
      "Epoch [3/10], Step [340], Loss: 0.000621026090811938\n",
      "Epoch [3/10], Step [350], Loss: 0.0006729871965944767\n",
      "Epoch [3/10], Step [360], Loss: 0.0005474964273162186\n",
      "Epoch [3/10], Step [370], Loss: 0.0005389656871557236\n",
      "Epoch [3/10], Step [380], Loss: 0.0007876466261222959\n",
      "Epoch [3/10], Step [390], Loss: 0.000533074198756367\n",
      "Epoch [3/10], Step [400], Loss: 0.0003365684824530035\n",
      "Epoch [3/10], Step [410], Loss: 0.0013815602287650108\n",
      "Epoch [3/10], Step [420], Loss: 0.007818853482604027\n",
      "Epoch [3/10], Step [430], Loss: 0.0023604147136211395\n",
      "Epoch [3/10], Step [440], Loss: 0.0011502535780891776\n",
      "Epoch [3/10], Step [450], Loss: 0.0011912976624444127\n",
      "Epoch [3/10], Step [460], Loss: 0.001200896454975009\n",
      "Epoch [3/10], Step [470], Loss: 0.0030221324414014816\n",
      "Epoch [3/10], Step [480], Loss: 0.005134176928550005\n",
      "Epoch [3/10], Step [490], Loss: 0.0007507531554438174\n",
      "Epoch [4/10], Step [0], Loss: 0.0006788844475522637\n",
      "Epoch [4/10], Step [10], Loss: 0.002907820977270603\n",
      "Epoch [4/10], Step [20], Loss: 0.0030474597588181496\n",
      "Epoch [4/10], Step [30], Loss: 0.0057569704949855804\n",
      "Epoch [4/10], Step [40], Loss: 0.003063277108594775\n",
      "Epoch [4/10], Step [50], Loss: 0.002991408109664917\n",
      "Epoch [4/10], Step [60], Loss: 0.0008069018367677927\n",
      "Epoch [4/10], Step [70], Loss: 0.000590799143537879\n",
      "Epoch [4/10], Step [80], Loss: 0.0009831673232838511\n",
      "Epoch [4/10], Step [90], Loss: 0.0004930389695800841\n",
      "Epoch [4/10], Step [100], Loss: 0.0006681744125671685\n",
      "Epoch [4/10], Step [110], Loss: 0.00038979723467491567\n",
      "Epoch [4/10], Step [120], Loss: 0.0008105573942884803\n",
      "Epoch [4/10], Step [130], Loss: 0.0003365356824360788\n",
      "Epoch [4/10], Step [140], Loss: 0.0062746042385697365\n",
      "Epoch [4/10], Step [150], Loss: 0.0004543475806713104\n",
      "Epoch [4/10], Step [160], Loss: 0.00040046792128123343\n",
      "Epoch [4/10], Step [170], Loss: 0.0012658562045544386\n",
      "Epoch [4/10], Step [180], Loss: 0.0006564719951711595\n",
      "Epoch [4/10], Step [190], Loss: 0.00033932982478290796\n",
      "Epoch [4/10], Step [200], Loss: 0.00023004178365226835\n",
      "Epoch [4/10], Step [210], Loss: 0.0003993780119344592\n",
      "Epoch [4/10], Step [220], Loss: 0.00036372095928527415\n",
      "Epoch [4/10], Step [230], Loss: 0.0004914642195217311\n",
      "Epoch [4/10], Step [240], Loss: 0.0008840296068228781\n",
      "Epoch [4/10], Step [250], Loss: 0.0002531578647904098\n",
      "Epoch [4/10], Step [260], Loss: 0.0009660247014835477\n",
      "Epoch [4/10], Step [270], Loss: 0.00044355489080771804\n",
      "Epoch [4/10], Step [280], Loss: 0.0003360637347213924\n",
      "Epoch [4/10], Step [290], Loss: 0.00030288437847048044\n",
      "Epoch [4/10], Step [300], Loss: 0.005558975040912628\n",
      "Epoch [4/10], Step [310], Loss: 0.0002768814447335899\n",
      "Epoch [4/10], Step [320], Loss: 0.0004064320819452405\n",
      "Epoch [4/10], Step [330], Loss: 0.0003655367763713002\n",
      "Epoch [4/10], Step [340], Loss: 0.00024028265033848584\n",
      "Epoch [4/10], Step [350], Loss: 0.0013307114131748676\n",
      "Epoch [4/10], Step [360], Loss: 0.008620561100542545\n",
      "Epoch [4/10], Step [370], Loss: 0.00036821817047894\n",
      "Epoch [4/10], Step [380], Loss: 0.0009860019199550152\n",
      "Epoch [4/10], Step [390], Loss: 0.005685620941221714\n",
      "Epoch [4/10], Step [400], Loss: 0.006008841097354889\n",
      "Epoch [4/10], Step [410], Loss: 0.00019471003906801343\n",
      "Epoch [4/10], Step [420], Loss: 0.0008074759971350431\n",
      "Epoch [4/10], Step [430], Loss: 0.0008274805149994791\n",
      "Epoch [4/10], Step [440], Loss: 0.0004989319713786244\n",
      "Epoch [4/10], Step [450], Loss: 0.019712375476956367\n",
      "Epoch [4/10], Step [460], Loss: 0.0005016719223931432\n",
      "Epoch [4/10], Step [470], Loss: 0.0012657037004828453\n",
      "Epoch [4/10], Step [480], Loss: 0.0017462966497987509\n",
      "Epoch [4/10], Step [490], Loss: 0.000488807971123606\n",
      "Epoch [5/10], Step [0], Loss: 0.00042000540997833014\n",
      "Epoch [5/10], Step [10], Loss: 0.0006609170814044774\n",
      "Epoch [5/10], Step [20], Loss: 0.00041088563739322126\n",
      "Epoch [5/10], Step [30], Loss: 0.0006593698635697365\n",
      "Epoch [5/10], Step [40], Loss: 0.000454167922725901\n",
      "Epoch [5/10], Step [50], Loss: 0.0005595001275651157\n",
      "Epoch [5/10], Step [60], Loss: 0.0005243761697784066\n",
      "Epoch [5/10], Step [70], Loss: 0.00035653519444167614\n",
      "Epoch [5/10], Step [80], Loss: 0.0002276350132888183\n",
      "Epoch [5/10], Step [90], Loss: 0.0009597348980605602\n",
      "Epoch [5/10], Step [100], Loss: 0.0005685826181434095\n",
      "Epoch [5/10], Step [110], Loss: 0.00034647772554308176\n",
      "Epoch [5/10], Step [120], Loss: 0.006060971412807703\n",
      "Epoch [5/10], Step [130], Loss: 0.0003076435823459178\n",
      "Epoch [5/10], Step [140], Loss: 0.0005323180230334401\n",
      "Epoch [5/10], Step [150], Loss: 0.0002566176117397845\n",
      "Epoch [5/10], Step [160], Loss: 0.00031136319739744067\n",
      "Epoch [5/10], Step [170], Loss: 0.0003016064001712948\n",
      "Epoch [5/10], Step [180], Loss: 0.0010853761341422796\n",
      "Epoch [5/10], Step [190], Loss: 0.00875240657478571\n",
      "Epoch [5/10], Step [200], Loss: 0.0029412300791591406\n",
      "Epoch [5/10], Step [210], Loss: 0.00035853771260008216\n",
      "Epoch [5/10], Step [220], Loss: 0.0029470024164766073\n",
      "Epoch [5/10], Step [230], Loss: 0.00067853182554245\n",
      "Epoch [5/10], Step [240], Loss: 0.00047926633851602674\n",
      "Epoch [5/10], Step [250], Loss: 0.000499925808981061\n",
      "Epoch [5/10], Step [260], Loss: 0.0006406427128240466\n",
      "Epoch [5/10], Step [270], Loss: 0.0015986028593033552\n",
      "Epoch [5/10], Step [280], Loss: 0.000338170793838799\n",
      "Epoch [5/10], Step [290], Loss: 0.0008293431019410491\n",
      "Epoch [5/10], Step [300], Loss: 0.0005377838388085365\n",
      "Epoch [5/10], Step [310], Loss: 0.0028941002674400806\n",
      "Epoch [5/10], Step [320], Loss: 0.00030933902598917484\n",
      "Epoch [5/10], Step [330], Loss: 0.0010678875260055065\n",
      "Epoch [5/10], Step [340], Loss: 0.005580872762948275\n",
      "Epoch [5/10], Step [350], Loss: 0.003032269887626171\n",
      "Epoch [5/10], Step [360], Loss: 0.0005096887471154332\n",
      "Epoch [5/10], Step [370], Loss: 0.00045978117850609124\n",
      "Epoch [5/10], Step [380], Loss: 0.0017246888019144535\n",
      "Epoch [5/10], Step [390], Loss: 0.0005275216535665095\n",
      "Epoch [5/10], Step [400], Loss: 0.0001840621407609433\n",
      "Epoch [5/10], Step [410], Loss: 0.00032640856807120144\n",
      "Epoch [5/10], Step [420], Loss: 0.001197231700643897\n",
      "Epoch [5/10], Step [430], Loss: 0.0002500240516383201\n",
      "Epoch [5/10], Step [440], Loss: 0.0005359513452276587\n",
      "Epoch [5/10], Step [450], Loss: 0.0038574112113565207\n",
      "Epoch [5/10], Step [460], Loss: 0.0003172907745465636\n",
      "Epoch [5/10], Step [470], Loss: 0.03276148810982704\n",
      "Epoch [5/10], Step [480], Loss: 0.0021163399796932936\n",
      "Epoch [5/10], Step [490], Loss: 0.0013613677583634853\n",
      "Epoch [6/10], Step [0], Loss: 0.0009630853310227394\n",
      "Epoch [6/10], Step [10], Loss: 0.00844288058578968\n",
      "Epoch [6/10], Step [20], Loss: 0.0004323254688642919\n",
      "Epoch [6/10], Step [30], Loss: 0.0004400082107167691\n",
      "Epoch [6/10], Step [40], Loss: 0.0003780945553444326\n",
      "Epoch [6/10], Step [50], Loss: 0.0018046607729047537\n",
      "Epoch [6/10], Step [60], Loss: 0.001814897870644927\n",
      "Epoch [6/10], Step [70], Loss: 0.0005505916196852922\n",
      "Epoch [6/10], Step [80], Loss: 0.005708510521799326\n",
      "Epoch [6/10], Step [90], Loss: 0.0004171552136540413\n",
      "Epoch [6/10], Step [100], Loss: 0.003862102050334215\n",
      "Epoch [6/10], Step [110], Loss: 0.0011859817896038294\n",
      "Epoch [6/10], Step [120], Loss: 0.00045626063365489244\n",
      "Epoch [6/10], Step [130], Loss: 0.0018906568875536323\n",
      "Epoch [6/10], Step [140], Loss: 0.0038928461726754904\n",
      "Epoch [6/10], Step [150], Loss: 0.00031565147219225764\n",
      "Epoch [6/10], Step [160], Loss: 0.00031925784423947334\n",
      "Epoch [6/10], Step [170], Loss: 0.0004068912530783564\n",
      "Epoch [6/10], Step [180], Loss: 0.0007499527418985963\n",
      "Epoch [6/10], Step [190], Loss: 0.0004365422064438462\n",
      "Epoch [6/10], Step [200], Loss: 0.00038468395359814167\n",
      "Epoch [6/10], Step [210], Loss: 0.00019499723566696048\n",
      "Epoch [6/10], Step [220], Loss: 0.0033577836584299803\n",
      "Epoch [6/10], Step [230], Loss: 0.000694675138220191\n",
      "Epoch [6/10], Step [240], Loss: 0.0002844748378265649\n",
      "Epoch [6/10], Step [250], Loss: 0.01708066090941429\n",
      "Epoch [6/10], Step [260], Loss: 0.00036720617208629847\n",
      "Epoch [6/10], Step [270], Loss: 0.0005574991810135543\n",
      "Epoch [6/10], Step [280], Loss: 0.0010538024362176657\n",
      "Epoch [6/10], Step [290], Loss: 0.0008430657908320427\n",
      "Epoch [6/10], Step [300], Loss: 0.001361260307021439\n",
      "Epoch [6/10], Step [310], Loss: 0.00889411848038435\n",
      "Epoch [6/10], Step [320], Loss: 0.0005363929667510092\n",
      "Epoch [6/10], Step [330], Loss: 0.0016535031609237194\n",
      "Epoch [6/10], Step [340], Loss: 0.003974916413426399\n",
      "Epoch [6/10], Step [350], Loss: 0.0005241166800260544\n",
      "Epoch [6/10], Step [360], Loss: 0.0008789554703980684\n",
      "Epoch [6/10], Step [370], Loss: 0.002305190311744809\n",
      "Epoch [6/10], Step [380], Loss: 0.0002314597077202052\n",
      "Epoch [6/10], Step [390], Loss: 0.0010954494355246425\n",
      "Epoch [6/10], Step [400], Loss: 0.008456300012767315\n",
      "Epoch [6/10], Step [410], Loss: 0.0009628296829760075\n",
      "Epoch [6/10], Step [420], Loss: 0.00027762187528423965\n",
      "Epoch [6/10], Step [430], Loss: 0.0011062083067372441\n",
      "Epoch [6/10], Step [440], Loss: 0.0003119412576779723\n",
      "Epoch [6/10], Step [450], Loss: 0.0005887307343073189\n",
      "Epoch [6/10], Step [460], Loss: 0.0004560709639918059\n",
      "Epoch [6/10], Step [470], Loss: 0.00038203399162739515\n",
      "Epoch [6/10], Step [480], Loss: 0.0006042859167791903\n",
      "Epoch [6/10], Step [490], Loss: 0.0007907765684649348\n",
      "Epoch [7/10], Step [0], Loss: 0.0013513017911463976\n",
      "Epoch [7/10], Step [10], Loss: 0.0012127237860113382\n",
      "Epoch [7/10], Step [20], Loss: 0.001604015240445733\n",
      "Epoch [7/10], Step [30], Loss: 0.0006587861571460962\n",
      "Epoch [7/10], Step [40], Loss: 0.0004933653981424868\n",
      "Epoch [7/10], Step [50], Loss: 0.00030280597275123\n",
      "Epoch [7/10], Step [60], Loss: 0.00027111906092613935\n",
      "Epoch [7/10], Step [70], Loss: 0.0005717244930565357\n",
      "Epoch [7/10], Step [80], Loss: 0.00020833726739510894\n",
      "Epoch [7/10], Step [90], Loss: 0.0012502754107117653\n",
      "Epoch [7/10], Step [100], Loss: 0.0012693445896729827\n",
      "Epoch [7/10], Step [110], Loss: 0.00021594065765384585\n",
      "Epoch [7/10], Step [120], Loss: 0.00033945360337384045\n",
      "Epoch [7/10], Step [130], Loss: 0.00017315955483354628\n",
      "Epoch [7/10], Step [140], Loss: 0.0018123439513146877\n",
      "Epoch [7/10], Step [150], Loss: 0.00865949783474207\n",
      "Epoch [7/10], Step [160], Loss: 0.0010709562338888645\n",
      "Epoch [7/10], Step [170], Loss: 0.00047287216875702143\n",
      "Epoch [7/10], Step [180], Loss: 0.0012303860858082771\n",
      "Epoch [7/10], Step [190], Loss: 0.00041348667582497\n",
      "Epoch [7/10], Step [200], Loss: 0.00033849477767944336\n",
      "Epoch [7/10], Step [210], Loss: 0.0007311649387702346\n",
      "Epoch [7/10], Step [220], Loss: 0.0003422408481128514\n",
      "Epoch [7/10], Step [230], Loss: 0.0003217117628082633\n",
      "Epoch [7/10], Step [240], Loss: 0.01011522114276886\n",
      "Epoch [7/10], Step [250], Loss: 0.0002846164570655674\n",
      "Epoch [7/10], Step [260], Loss: 0.00036021211417391896\n",
      "Epoch [7/10], Step [270], Loss: 0.0035319561138749123\n",
      "Epoch [7/10], Step [280], Loss: 0.0003220879298169166\n",
      "Epoch [7/10], Step [290], Loss: 0.0005126576288603246\n",
      "Epoch [7/10], Step [300], Loss: 0.00037876266287639737\n",
      "Epoch [7/10], Step [310], Loss: 0.0056836968287825584\n",
      "Epoch [7/10], Step [320], Loss: 0.0010155739728361368\n",
      "Epoch [7/10], Step [330], Loss: 0.000844816502649337\n",
      "Epoch [7/10], Step [340], Loss: 0.0006692712777294219\n",
      "Epoch [7/10], Step [350], Loss: 0.0007055819733068347\n",
      "Epoch [7/10], Step [360], Loss: 0.00040310522308573127\n",
      "Epoch [7/10], Step [370], Loss: 0.002606443827971816\n",
      "Epoch [7/10], Step [380], Loss: 0.0004131002351641655\n",
      "Epoch [7/10], Step [390], Loss: 0.0003975912113673985\n",
      "Epoch [7/10], Step [400], Loss: 0.0012333703925833106\n",
      "Epoch [7/10], Step [410], Loss: 0.0006820012349635363\n",
      "Epoch [7/10], Step [420], Loss: 0.0007213680655695498\n",
      "Epoch [7/10], Step [430], Loss: 0.0002956753014586866\n",
      "Epoch [7/10], Step [440], Loss: 0.001744971377775073\n",
      "Epoch [7/10], Step [450], Loss: 0.00020822262740693986\n",
      "Epoch [7/10], Step [460], Loss: 0.00019581701781135052\n",
      "Epoch [7/10], Step [470], Loss: 0.0037804306484758854\n",
      "Epoch [7/10], Step [480], Loss: 0.0023553441278636456\n",
      "Epoch [7/10], Step [490], Loss: 0.0013468090910464525\n",
      "Epoch [8/10], Step [0], Loss: 0.0004142398538533598\n",
      "Epoch [8/10], Step [10], Loss: 0.000652325979899615\n",
      "Epoch [8/10], Step [20], Loss: 0.002439535688608885\n",
      "Epoch [8/10], Step [30], Loss: 0.0009143830393441021\n",
      "Epoch [8/10], Step [40], Loss: 0.0005873665795661509\n",
      "Epoch [8/10], Step [50], Loss: 0.0013044513761997223\n",
      "Epoch [8/10], Step [60], Loss: 0.00040006774361245334\n",
      "Epoch [8/10], Step [70], Loss: 0.0003946694196201861\n",
      "Epoch [8/10], Step [80], Loss: 0.0005477957893162966\n",
      "Epoch [8/10], Step [90], Loss: 0.00048251455882564187\n",
      "Epoch [8/10], Step [100], Loss: 0.0011797482147812843\n",
      "Epoch [8/10], Step [110], Loss: 0.00064581457991153\n",
      "Epoch [8/10], Step [120], Loss: 0.0003750066098291427\n",
      "Epoch [8/10], Step [130], Loss: 0.0006223700474947691\n",
      "Epoch [8/10], Step [140], Loss: 0.00024749781005084515\n",
      "Epoch [8/10], Step [150], Loss: 0.0007025192026048899\n",
      "Epoch [8/10], Step [160], Loss: 0.0020209155045449734\n",
      "Epoch [8/10], Step [170], Loss: 0.0006014318205416203\n",
      "Epoch [8/10], Step [180], Loss: 0.00043914932757616043\n",
      "Epoch [8/10], Step [190], Loss: 0.0007736358093097806\n",
      "Epoch [8/10], Step [200], Loss: 0.0003813721996266395\n",
      "Epoch [8/10], Step [210], Loss: 0.000352424627635628\n",
      "Epoch [8/10], Step [220], Loss: 0.0005454422207549214\n",
      "Epoch [8/10], Step [230], Loss: 0.0002874352503567934\n",
      "Epoch [8/10], Step [240], Loss: 0.0003930749953724444\n",
      "Epoch [8/10], Step [250], Loss: 0.0004252290236763656\n",
      "Epoch [8/10], Step [260], Loss: 0.00027496545226313174\n",
      "Epoch [8/10], Step [270], Loss: 0.000437155831605196\n",
      "Epoch [8/10], Step [280], Loss: 0.003227726323530078\n",
      "Epoch [8/10], Step [290], Loss: 0.0017315028235316277\n",
      "Epoch [8/10], Step [300], Loss: 0.0021534881088882685\n",
      "Epoch [8/10], Step [310], Loss: 0.0002740047057159245\n",
      "Epoch [8/10], Step [320], Loss: 0.0008072821656242013\n",
      "Epoch [8/10], Step [330], Loss: 0.0025121334474533796\n",
      "Epoch [8/10], Step [340], Loss: 0.00024809635942801833\n",
      "Epoch [8/10], Step [350], Loss: 0.001177379279397428\n",
      "Epoch [8/10], Step [360], Loss: 0.00023490989406127483\n",
      "Epoch [8/10], Step [370], Loss: 0.00026370942941866815\n",
      "Epoch [8/10], Step [380], Loss: 0.0009412018698640168\n",
      "Epoch [8/10], Step [390], Loss: 0.0027258635964244604\n",
      "Epoch [8/10], Step [400], Loss: 0.0002948623150587082\n",
      "Epoch [8/10], Step [410], Loss: 0.0007323890458792448\n",
      "Epoch [8/10], Step [420], Loss: 0.00034242073888890445\n",
      "Epoch [8/10], Step [430], Loss: 0.0003528420929796994\n",
      "Epoch [8/10], Step [440], Loss: 0.00022074654407333583\n",
      "Epoch [8/10], Step [450], Loss: 0.0018057894194498658\n",
      "Epoch [8/10], Step [460], Loss: 0.005878014024347067\n",
      "Epoch [8/10], Step [470], Loss: 0.00030727623379789293\n",
      "Epoch [8/10], Step [480], Loss: 0.00019035927834920585\n",
      "Epoch [8/10], Step [490], Loss: 0.0012232601875439286\n",
      "Epoch [9/10], Step [0], Loss: 0.002308586612343788\n",
      "Epoch [9/10], Step [10], Loss: 0.0010302863083779812\n",
      "Epoch [9/10], Step [20], Loss: 0.001352978521026671\n",
      "Epoch [9/10], Step [30], Loss: 0.0004106323467567563\n",
      "Epoch [9/10], Step [40], Loss: 0.0004691240901593119\n",
      "Epoch [9/10], Step [50], Loss: 0.0115733053535223\n",
      "Epoch [9/10], Step [60], Loss: 0.0005563337472267449\n",
      "Epoch [9/10], Step [70], Loss: 0.0008542779251001775\n",
      "Epoch [9/10], Step [80], Loss: 0.00034238738589920104\n",
      "Epoch [9/10], Step [90], Loss: 0.0005036963848397136\n",
      "Epoch [9/10], Step [100], Loss: 0.000642788945697248\n",
      "Epoch [9/10], Step [110], Loss: 0.0014605821343138814\n",
      "Epoch [9/10], Step [120], Loss: 0.0003305650025140494\n",
      "Epoch [9/10], Step [130], Loss: 0.003584811929613352\n",
      "Epoch [9/10], Step [140], Loss: 0.0010538423666730523\n",
      "Epoch [9/10], Step [150], Loss: 0.0005652693216688931\n",
      "Epoch [9/10], Step [160], Loss: 0.004659509286284447\n",
      "Epoch [9/10], Step [170], Loss: 0.000613448559306562\n",
      "Epoch [9/10], Step [180], Loss: 0.0008480942924506962\n",
      "Epoch [9/10], Step [190], Loss: 0.000385862251278013\n",
      "Epoch [9/10], Step [200], Loss: 0.0003649778664112091\n",
      "Epoch [9/10], Step [210], Loss: 0.00036473333602771163\n",
      "Epoch [9/10], Step [220], Loss: 0.00030935410177335143\n",
      "Epoch [9/10], Step [230], Loss: 0.0013955363538116217\n",
      "Epoch [9/10], Step [240], Loss: 0.003079624380916357\n",
      "Epoch [9/10], Step [250], Loss: 0.001631296705454588\n",
      "Epoch [9/10], Step [260], Loss: 0.0008435480995103717\n",
      "Epoch [9/10], Step [270], Loss: 0.0045168716460466385\n",
      "Epoch [9/10], Step [280], Loss: 0.000835169164929539\n",
      "Epoch [9/10], Step [290], Loss: 0.0002733509463723749\n",
      "Epoch [9/10], Step [300], Loss: 0.016518646851181984\n",
      "Epoch [9/10], Step [310], Loss: 0.00027842706185765564\n",
      "Epoch [9/10], Step [320], Loss: 0.0011072569759562612\n",
      "Epoch [9/10], Step [330], Loss: 0.0003214104217477143\n",
      "Epoch [9/10], Step [340], Loss: 0.005130855366587639\n",
      "Epoch [9/10], Step [350], Loss: 0.00042466260492801666\n",
      "Epoch [9/10], Step [360], Loss: 0.0014432332245633006\n",
      "Epoch [9/10], Step [370], Loss: 0.0003937483124900609\n",
      "Epoch [9/10], Step [380], Loss: 0.0003344200667925179\n",
      "Epoch [9/10], Step [390], Loss: 0.0008706951048225164\n",
      "Epoch [9/10], Step [400], Loss: 0.0006193015142343938\n",
      "Epoch [9/10], Step [410], Loss: 0.0007204304565675557\n",
      "Epoch [9/10], Step [420], Loss: 0.0004231795319356024\n",
      "Epoch [9/10], Step [430], Loss: 0.00035552767803892493\n",
      "Epoch [9/10], Step [440], Loss: 0.0009070750675164163\n",
      "Epoch [9/10], Step [450], Loss: 0.0005065015866421163\n",
      "Epoch [9/10], Step [460], Loss: 0.0011947548482567072\n",
      "Epoch [9/10], Step [470], Loss: 0.00035915797343477607\n",
      "Epoch [9/10], Step [480], Loss: 0.006014461163431406\n",
      "Epoch [9/10], Step [490], Loss: 0.0007585657294839621\n",
      "Epoch [10/10], Step [0], Loss: 0.0004541667294688523\n",
      "Epoch [10/10], Step [10], Loss: 0.0009016419062390924\n",
      "Epoch [10/10], Step [20], Loss: 0.0015116045251488686\n",
      "Epoch [10/10], Step [30], Loss: 0.0001555749768158421\n",
      "Epoch [10/10], Step [40], Loss: 0.00039172530523501337\n",
      "Epoch [10/10], Step [50], Loss: 0.0011005713604390621\n",
      "Epoch [10/10], Step [60], Loss: 0.0028949975967407227\n",
      "Epoch [10/10], Step [70], Loss: 0.0003161013883072883\n",
      "Epoch [10/10], Step [80], Loss: 0.0006488458602689207\n",
      "Epoch [10/10], Step [90], Loss: 0.00024696963373571634\n",
      "Epoch [10/10], Step [100], Loss: 0.0030270335264503956\n",
      "Epoch [10/10], Step [110], Loss: 0.0015708553837612271\n",
      "Epoch [10/10], Step [120], Loss: 0.00033170072128996253\n",
      "Epoch [10/10], Step [130], Loss: 0.007477014325559139\n",
      "Epoch [10/10], Step [140], Loss: 0.0003729686723090708\n",
      "Epoch [10/10], Step [150], Loss: 0.0016897410387173295\n",
      "Epoch [10/10], Step [160], Loss: 0.0012321220710873604\n",
      "Epoch [10/10], Step [170], Loss: 0.004475461784750223\n",
      "Epoch [10/10], Step [180], Loss: 0.00043684340198524296\n",
      "Epoch [10/10], Step [190], Loss: 0.00031339636188931763\n",
      "Epoch [10/10], Step [200], Loss: 0.0007060978678055108\n",
      "Epoch [10/10], Step [210], Loss: 0.0003669987490866333\n",
      "Epoch [10/10], Step [220], Loss: 0.0005483629647642374\n",
      "Epoch [10/10], Step [230], Loss: 0.00036918511614203453\n",
      "Epoch [10/10], Step [240], Loss: 0.00029864179668948054\n",
      "Epoch [10/10], Step [250], Loss: 0.00045155544648878276\n",
      "Epoch [10/10], Step [260], Loss: 0.0024409708566963673\n",
      "Epoch [10/10], Step [270], Loss: 0.0008157978300005198\n",
      "Epoch [10/10], Step [280], Loss: 0.0008040282991714776\n",
      "Epoch [10/10], Step [290], Loss: 0.0008913440397009254\n",
      "Epoch [10/10], Step [300], Loss: 0.0007097374182194471\n",
      "Epoch [10/10], Step [310], Loss: 0.014557169750332832\n",
      "Epoch [10/10], Step [320], Loss: 0.0003069335944019258\n",
      "Epoch [10/10], Step [330], Loss: 0.0019605786073952913\n",
      "Epoch [10/10], Step [340], Loss: 0.003349158214405179\n",
      "Epoch [10/10], Step [350], Loss: 0.00034607804263941944\n",
      "Epoch [10/10], Step [360], Loss: 0.0005571568617597222\n",
      "Epoch [10/10], Step [370], Loss: 0.006333764176815748\n",
      "Epoch [10/10], Step [380], Loss: 0.0007105455733835697\n",
      "Epoch [10/10], Step [390], Loss: 0.0027185198850929737\n",
      "Epoch [10/10], Step [400], Loss: 0.00039711984572932124\n",
      "Epoch [10/10], Step [410], Loss: 0.00031397221027873456\n",
      "Epoch [10/10], Step [420], Loss: 0.0014193668030202389\n",
      "Epoch [10/10], Step [430], Loss: 0.002586456248536706\n",
      "Epoch [10/10], Step [440], Loss: 0.000941186910495162\n",
      "Epoch [10/10], Step [450], Loss: 0.00288753560744226\n",
      "Epoch [10/10], Step [460], Loss: 0.00027860235422849655\n",
      "Epoch [10/10], Step [470], Loss: 0.00057533651124686\n",
      "Epoch [10/10], Step [480], Loss: 0.0018059416906908154\n",
      "Epoch [10/10], Step [490], Loss: 0.002526756376028061\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def get_faster_rcnn_model(num_classes):\n",
    "    # Load a pre-trained Faster R-CNN model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one (for your custom number of classes)\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class IconDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotations_dir, transforms=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_files = list(sorted(os.listdir(images_dir)))\n",
    "        self.annotation_files = list(sorted(os.listdir(annotations_dir)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images and annotations\n",
    "        image_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        annotation_path = os.path.join(self.annotations_dir, self.annotation_files[idx])\n",
    "        \n",
    "        # Open the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load the annotations (assuming COCO format, or adjust this part to your format)\n",
    "        # You might need to write custom logic here to load your annotations\n",
    "        # Example: loading annotations as a dict with bounding boxes and labels\n",
    "        target = self.load_annotation(annotation_path)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def load_annotation(self, annotation_path):\n",
    "        # Custom logic to load your annotation\n",
    "        # You can parse a JSON, XML, or whatever format you're using\n",
    "        annotation = {\n",
    "            'boxes': torch.tensor([[0, 0, 10, 10]]),  # Example bounding box\n",
    "            'labels': torch.tensor([1])  # Example label\n",
    "        }\n",
    "        return annotation\n",
    "\n",
    "\n",
    "# Fungsi collate_fn untuk batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Fungsi untuk melatih model\n",
    "def train_faster_rcnn(model, dataset, device, num_epochs=10, lr=0.005):\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # Scheduler untuk menurunkan learning rate\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        i = 0\n",
    "        for images, targets in DataLoader(dataset, batch_size=2, collate_fn=collate_fn, shuffle=True):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            # Compute total loss\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Backward pass\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}], Loss: {losses.item()}\")\n",
    "            i += 1\n",
    "\n",
    "        # Update the learning rate\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Dataset paths\n",
    "images_dir = 'generated_dataset/images'\n",
    "annotations_dir = 'generated_dataset/annotations'\n",
    "\n",
    "# Transformasi dataset\n",
    "transforms = T.Compose([T.ToTensor()])\n",
    "\n",
    "# Dataset instance\n",
    "dataset = IconDataset(images_dir, annotations_dir, transforms)\n",
    "\n",
    "# Faster R-CNN model (num_classes = jumlah kelas + background class)\n",
    "num_classes = len(os.listdir(dataset_dir)) + 1  # 1 for background\n",
    "model = get_faster_rcnn_model(num_classes)\n",
    "\n",
    "# Training\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_faster_rcnn(model, dataset, device, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e8f50df-a5d0-4c38-b946-62a543ec65e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model disimpan ke model_fasterrcnn.pth\n"
     ]
    }
   ],
   "source": [
    "# Path untuk menyimpan model\n",
    "model_save_path = 'model_fasterrcnn.pth'\n",
    "\n",
    "# Simpan state_dict model\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model disimpan ke {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa010e87-d171-4eb1-aa41-7041a4c38d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah kelas yang ditemukan: 105\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Folder anotasi\n",
    "annotations_dir = 'generated_dataset/annotations'\n",
    "\n",
    "# Fungsi untuk mendapatkan jumlah kelas dari file anotasi\n",
    "def get_num_classes_from_annotations(annotations_dir):\n",
    "    class_set = set()\n",
    "    \n",
    "    # Loop melalui semua file JSON di folder anotasi\n",
    "    for annotation_file in os.listdir(annotations_dir):\n",
    "        annotation_path = os.path.join(annotations_dir, annotation_file)\n",
    "        \n",
    "        # Baca file JSON\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            annotation_data = json.load(f)\n",
    "            \n",
    "            # Tambahkan semua kelas yang ditemukan ke set\n",
    "            class_set.update(annotation_data['classes'])\n",
    "    \n",
    "    return len(class_set)\n",
    "\n",
    "# Ambil jumlah kelas dari file anotasi\n",
    "num_classes = get_num_classes_from_annotations(annotations_dir)\n",
    "print(f\"Jumlah kelas yang ditemukan: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81a5f7db-4385-4def-b8cf-07c9d26f5c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17506/3253445658.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_fasterrcnn.pth'))  # Ganti dengan path checkpoint Anda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FasterRCNN:\n\tsize mismatch for roi_heads.box_predictor.cls_score.weight: copying a param with shape torch.Size([106, 1024]) from checkpoint, the shape in current model is torch.Size([105, 1024]).\n\tsize mismatch for roi_heads.box_predictor.cls_score.bias: copying a param with shape torch.Size([106]) from checkpoint, the shape in current model is torch.Size([105]).\n\tsize mismatch for roi_heads.box_predictor.bbox_pred.weight: copying a param with shape torch.Size([424, 1024]) from checkpoint, the shape in current model is torch.Size([420, 1024]).\n\tsize mismatch for roi_heads.box_predictor.bbox_pred.bias: copying a param with shape torch.Size([424]) from checkpoint, the shape in current model is torch.Size([420]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Muat checkpoint model (jika sudah dilatih sebelumnya)\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_fasterrcnn.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# Ganti dengan path checkpoint Anda\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Path ke gambar yang akan diprediksi\u001b[39;00m\n\u001b[1;32m     76\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Ganti dengan path gambar Anda\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FasterRCNN:\n\tsize mismatch for roi_heads.box_predictor.cls_score.weight: copying a param with shape torch.Size([106, 1024]) from checkpoint, the shape in current model is torch.Size([105, 1024]).\n\tsize mismatch for roi_heads.box_predictor.cls_score.bias: copying a param with shape torch.Size([106]) from checkpoint, the shape in current model is torch.Size([105]).\n\tsize mismatch for roi_heads.box_predictor.bbox_pred.weight: copying a param with shape torch.Size([424, 1024]) from checkpoint, the shape in current model is torch.Size([420, 1024]).\n\tsize mismatch for roi_heads.box_predictor.bbox_pred.bias: copying a param with shape torch.Size([424]) from checkpoint, the shape in current model is torch.Size([420])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Fungsi untuk memuat model Faster R-CNN\n",
    "def get_faster_rcnn_model(num_classes):\n",
    "    # Load pre-trained Faster R-CNN model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one (for your custom number of classes)\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.FasterRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Fungsi untuk memuat model dengan parameter yang kompatibel\n",
    "def load_model_with_compatible_params(model, checkpoint_path):\n",
    "    # Muat state dict dari checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Ambil state dict dari model yang disimpan\n",
    "    model_state_dict = model.state_dict()\n",
    "    \n",
    "    # Update state dict model dengan checkpoint\n",
    "    # Hanya parameter yang cocok yang akan dimuat\n",
    "    model_state_dict.update({k: v for k, v in checkpoint.items() if k in model_state_dict})\n",
    "    \n",
    "    # Muat state dict yang telah diperbarui ke model\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    return model\n",
    "\n",
    "# Fungsi untuk melakukan prediksi\n",
    "def predict(model, image_path, device, threshold=0.5):\n",
    "    model.eval()  # Set model ke evaluasi mode\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Transformasi gambar menjadi tensor\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)  # Batch size of 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img_tensor)  # Dapatkan prediksi\n",
    "\n",
    "    # Ambil prediksi dengan confidence score di atas threshold\n",
    "    pred_boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    pred_scores = predictions[0]['scores'].cpu().numpy()\n",
    "    pred_labels = predictions[0]['labels'].cpu().numpy()\n",
    "\n",
    "    # Filter hasil berdasarkan threshold\n",
    "    high_conf_boxes = pred_boxes[pred_scores >= threshold]\n",
    "    high_conf_labels = pred_labels[pred_scores >= threshold]\n",
    "    high_conf_scores = pred_scores[pred_scores >= threshold]\n",
    "\n",
    "    return high_conf_boxes, high_conf_labels, high_conf_scores\n",
    "\n",
    "# Fungsi untuk menampilkan gambar dengan bounding box\n",
    "def display_prediction(image_path, boxes, labels, scores):\n",
    "    img = Image.open(image_path)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Buat bounding box di sekitar objek yang terdeteksi\n",
    "    for i, box in enumerate(boxes):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(xmin, ymin - 10, f'Label: {labels[i]}, Score: {scores[i]:.2f}', color='white', fontsize=12, \n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Contoh penggunaan\n",
    "if __name__ == \"__main__\":\n",
    "    # Jumlah kelas (misal, 106 kelas + background)\n",
    "    num_classes = 105  # Sesuaikan dengan dataset\n",
    "    \n",
    "    # Inisialisasi model\n",
    "    model = get_faster_rcnn_model(num_classes)\n",
    "    \n",
    "    # Gunakan GPU jika tersedia\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Muat checkpoint model (jika sudah dilatih sebelumnya) dengan parameter yang kompatibel\n",
    "    checkpoint_path = 'model_fasterrcnn.pth'  # Ganti dengan path checkpoint Anda\n",
    "    model = load_model_with_compatible_params(model, checkpoint_path)\n",
    "    \n",
    "    # Path ke gambar yang akan diprediksi\n",
    "    image_path = 'test_image.jpg'  # Ganti dengan path gambar Anda\n",
    "    \n",
    "    # Prediksi\n",
    "    boxes, labels, scores = predict(model, image_path, device, threshold=0.5)\n",
    "    \n",
    "    # Tampilkan hasil prediksi\n",
    "    display_prediction(image_path, boxes, labels, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3fa37-cb0f-45f1-8373-23706c727aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
